Sat Nov  2 16:23:03 +08 2024
CONDA_SHLVL=1
PBS_ENVIRONMENT=PBS_BATCH
LD_LIBRARY_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64:/app/apps/openmpi/4.1.2-hpe/lib:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/lib:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/lib:/app/apps/local/lib64:/app/apps/local/lib
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:
CONDA_EXE=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin/conda
nvidia_group_id=278
PBS_O_LANG=en_US.UTF-8
PRGENVMODULES=PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pgi:PrrgEnv-amd
max_steps=20
SSH_CONNECTION=10.103.62.245 56260 10.103.153.100 22
LANG=en_US.UTF-8
TZ=Asia/Singapore
HISTCONTROL=ignoredups
DISPLAY=localhost:11.0
HOSTNAME=x1000c2s1b0n1
OLDPWD=/scratch/users/industry/ai-hpc/apacsc41/workdir/llama/out/finetune/new/adapter256
PBS_O_HOME=/home/users/industry/ai-hpc/apacsc41
PBS_JOBID=8559611.pbs101
ENVIRONMENT=BATCH
global_batch_size=256
PBS_JOBNAME=lora-1.nodes2.GBS256.MBS64
FPATH=:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
NCPUS=64
CONDA_PREFIX=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3
PBS_O_PATH=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/condabin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/home/users/industry/ai-hpc/apacsc41/.local/bin:/home/users/industry/ai-hpc/apacsc41/bin:/opt/cray/pe/pals/1.1.6/bin:/opt/cray/pe/craype/2.7.15/bin:/opt/cray/pe/cce/13.0.2/binutils/x86_64/x86_64-pc-linux-gnu/bin:/opt/cray/pe/cce/13.0.2/binutils/cross/x86_64-aarch64/bin:/opt/cray/pe/cce/13.0.2/utils/x86_64/bin:/opt/cray/pe/cce/13.0.2/bin:/opt/cray/pe/perftools/22.04.0/bin:/opt/cray/pe/papi/6.0.0.14/bin:/opt/cray/libfabric/1.11.0.4.125/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/cray/pe/bin
S_COLORS=auto
C3_RSH=ssh -oConnectTimeout=10 -oForwardX11=no
_CE_M=
XDG_SESSION_ID=80985
PE_OPENMPI_BASEDIR=/app/apps/openmpi/4.1.2-hpe
PBS_O_WORKDIR=/home/users/industry/ai-hpc/apacsc41/run/llama-run/new
USER=apacsc41
MODULE_VERSION=3.2.11.6
TOOLMODULES=apprentice:apprentice2:atp:ccdb:cdt:chapel:cray-cti:cray-lgdb:craypat:craypkg-gen:cray-R:cray-snplauncher:ddt:gdb:gdb4hpc:iobuf:papi:perftools:perftools-lite:python:stat:totalview:xt-craypat:xt-lgdb:xt-papi:xt-totalview
micro_batch_size=64
PBS_NODEFILE=/var/spool/pbs/aux/8559611.pbs101
KSH_AUTOLOAD=1
PE_FORTRAN_PKGCONFIG_LIBS=ompi-fort
PBS_TASKNUM=1
PWD=/home/users/industry/ai-hpc/apacsc41
TARGETMODULES=craype-accel-amd-gfx908:craype-accel-amd-gfx90a:craype-accel-host:craype-accel-nvidia70:craype-accel-nvidia80:craype-network-none:craype-network-ofi:craype-network-ucx:craype-x86-milan:craype-x86-milan-x:craype-x86-rome:craype-x86-trento
HOME=/home/users/industry/ai-hpc/apacsc41
CONDA_PYTHON_EXE=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin/python
PELOCAL_PRGENV=true
SSH_CLIENT=10.103.62.245 56260 22
CPATH=/app/apps/openmpi/4.1.2-hpe/include
PBS_MOMPORT=15003
OPENMPI_VERSION=4.1.2-hpe
XDG_DATA_DIRS=/home/users/industry/ai-hpc/apacsc41/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
PE_CXX_PKGCONFIG_LIBS=ompi-cxx
_CE_CONDA=
GSETTINGS_SCHEMA_DIR=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/share/glib-2.0/schemas
PBS_JOBCOOKIE=6C2380727071E63F5BB78B031464B2EF
PBS_O_SHELL=/bin/bash
TMPDIR=/var/tmp/pbs.8559611.pbs101
LIBRARY_PATH=/app/apps/openmpi/4.1.2-hpe/lib
PE_PKGCONFIG_LIBS=ompi
USERMODULES=acml:alps:apprentice:apprentice2:atp:blcr:cce:chapel:cray-ccdb:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-lgdb:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-compat:cray-netcdf:cray-netcdf-hdf5parallel:cray-openshmemx:cray-parallel-netcdf:craypat:craype:cray-petsc:cray-petsc-complex:craypkg-gen:cray-R:cray-shmem:cray-snplauncher:cray-tpsl:cray-trilinos:cudatoolkit:ddt:fftw:ga:gcc:hdf5:hdf5-parallel:intel:iobuf:java:lgdb:libfast:libsci_acc:mpich1:netcdf:netcdf-hdf5parallel:netcdf-nofsync:netcdf-nofsync-hdf5parallel:ntk:nvidia:onesided:papi:parallel-netcdf:pathscale:perftools:perftools-lite:petsc:petsc-complex:pgi:pmi:PrgEnv-amd:PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pathscale:PrgEnv-pgi:python:rocm-compiler:stat:totalview:tpsl:trilinos:xt-asyncpe:xt-craypat:xt-lgdb:xt-libsci:xt-mpich2:xt-mpt:xt-papi:xt-shmem:xt-totalview
LOADEDMODULES=openmpi/4.1.2-hpe:libfabric/1.11.0.4.125
LIBRARYMODULES=acml:alps:cray-dwarf:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-abi:cray-netcdf:cray-netcdf-hdf5parallel:cray-parallel-netcdf:cray-petsc:cray-petsc-complex:cray-shmem:cray-tpsl:cray-trilinos:cudatoolkit:fftw:ga:hdf5:hdf5-parallel:iobuf:libfast:netcdf:netcdf-hdf5parallel:ntk:onesided:papi:petsc:petsc-complex:pmi:tpsl:trilinos:xt-libsci:xt-mpich2:xt-mpt:xt-papi
CONDA_PROMPT_MODIFIER=(base) 
SSH_TTY=/dev/pts/1
PBS_O_QUEUE=normal
MAIL=/var/spool/mail/apacsc41
MOTDLOCK=1
CRAY_SITE_LIST_DIR=/etc/cray-pe.d/cray-modules
SHELL=/bin/bash
TERM=xterm
walltime=7201
USE_PCM_DB=2
CUDA_DEVICE_ORDER=PCI_BUS_ID
CUDA_VISIBLE_DEVICES=GPU-ebc16125-83b2-a031-7b51-dd2d1ed2af0e,GPU-2e171fbb-edfe-7767-067b-1f102446cb8f,GPU-636aff87-a12a-6f16-7986-68a70cc29c0c,GPU-db18ce5d-c813-616c-32fa-695d270202ea
SHLVL=4
LANGUAGE=en_US.UTF-8
PBS_O_HOST=asp2a-login-nscc01.head.cm.asp2a.nscc.sg
PBS_O_SYSTEM=Linux
MANPATH=/opt/cray/libfabric/1.11.0.4.125/share/man:/app/apps/openmpi/4.1.2-hpe/share/man:/opt/cray/pe/modules/3.2.11.6/share/man:/opt/cray/pe/modules/3.2.11.6/share/man::/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man
OSCAR_HOME=/opt/oscar
PBS_O_LOGNAME=apacsc41
PBS_NODENUM=0
GDK_BACKEND=x11
MODULEPATH=/opt/cray/pe/modulefiles:/opt/cray/modulefiles:/opt/modulefiles:/opt/cray/pe/craype-targets/default/modulefiles:/app/apps/modulefiles:/app/libs/modulefiles
PBS_JOBDIR=/home/users/industry/ai-hpc/apacsc41
LOGNAME=apacsc41
DBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-q51ZEXW4aU,guid=7810dc4d158830661feb15386725bef9
XDG_RUNTIME_DIR=/run/user/12353
nodes=2
MODULE_VERSION_STACK=3.2.11.6
PE_C_PKGCONFIG_LIBS=ompi-c
OPENMPI_DIR=/app/apps/openmpi/4.1.2-hpe
PATH=/opt/cray/libfabric/1.11.0.4.125/bin:/app/apps/openmpi/4.1.2-hpe/bin:/app/apps/openmpi/4.1.2-hpe/include:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/condabin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/home/users/industry/ai-hpc/apacsc41/.local/bin:/home/users/industry/ai-hpc/apacsc41/bin:/opt/cray/pe/cce/13.0.2/binutils/cross/x86_64-aarch64/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/pbs/bin:/sbin:/bin
_LMFILES_=/app/apps/modulefiles/openmpi/4.1.2-hpe:/opt/cray/modulefiles/libfabric/1.11.0.4.125
PBS_QUEUE=g4
MODULESHOME=/opt/cray/pe/modules/3.2.11.6
PKG_CONFIG_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64/pkgconfig:/app/apps/openmpi/4.1.2-hpe/lib/pkgconfig
CONDA_DEFAULT_ENV=base
HISTSIZE=1000
XML_CATALOG_FILES=file:///home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/etc/xml/catalog file:///etc/xml/catalog
OPENMPI_POST_LINK_OPTS=-L/app/apps/openmpi/4.1.2-hpe/lib -Wl,-rpath=/app/apps/openmpi/4.1.2-hpe/lib
PBS_O_MAIL=/var/spool/mail/apacsc41
LESSOPEN=||/usr/bin/lesspipe.sh %s
OMP_NUM_THREADS=64
LC_TIME=en_US.UTF-8
BASH_FUNC_module%%=() {  eval `/opt/cray/pe/modules/3.2.11.6/bin/modulecmd bash $*`
}
_=/usr/bin/env
x1000c2s1b0n1.hostmgmt2000.cm.asp2a.nscc.sg
x1000c2s7b0n0.hostmgmt2000.cm.asp2a.nscc.sg
mpirun -wdir /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama -output-filename /home/users/industry/ai-hpc/apacsc41/run/output/lora/lora-1.nodes2.GBS256.MBS64.8559611.pbs101 -map-by ppr:4:node -oversubscribe -report-bindings -x NCCL_DEBUG=INFO -x NCCL_NET_GDR_LEVEL=0 -x NCCL_IB_DISABLE=1 -mca pml ^ucx /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/bin/python /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py finetune_lora /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf --out_dir /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first --data JSON --data.json_path /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024 --config /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/lora.yaml --eval.final_validation=false --train.epochs=1 --devices=4 --num_nodes=2 --train.max_steps=20 --train.global_batch_size=256 --train.micro_batch_size=64
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x145e3fff92e0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x145466b7bd70>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x15121bb485f0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x152687a2e030>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c2s1b0n1:3320354] MCW rank 0 is not bound (or bound to all available processors)
[x1000c2s1b0n1:3320357] MCW rank 3 is not bound (or bound to all available processors)
[x1000c2s1b0n1:3320355] MCW rank 1 is not bound (or bound to all available processors)
[x1000c2s1b0n1:3320356] MCW rank 2 is not bound (or bound to all available processors)
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           x1000c2s1b0n1
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
[x1000c2s1b0n1:3320340] 7 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1000c2s1b0n1:3320340] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14a9c2b52030>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14d45d77dc10>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14d533ce39b0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14f8c38fc2f0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=256,
                    micro_batch_size=64,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c2s7b0n0:2841743] MCW rank 4 is not bound (or bound to all available processors)
[x1000c2s7b0n0:2841745] MCW rank 6 is not bound (or bound to all available processors)
[x1000c2s7b0n0:2841746] MCW rank 7 is not bound (or bound to all available processors)
[x1000c2s7b0n0:2841744] MCW rank 5 is not bound (or bound to all available processors)
All GPUs are fully connected via NVLink.
All GPUs are fully connected via NVLink.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Number of trainable parameters: 4,194,304
Number of non-trainable parameters: 6,738,415,616
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requi than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters  than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The fhave requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transfres_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14ollowing parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.lin.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. ormer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True aear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with nd False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
Number of trainable parameters: 4,194,304
Number of non-trainable parameters: 6,738,415,616
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.0 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.0.attn.attn.lora_A', 'transformer.h.0.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.0.norm_1.weight', 'transformer.h.0.norm_2.weight', 'transformer.h.0.attn.attn.linear.weight', 'transformer.h.0.attn.proj.linear.weight', 'transformer.h.0.mlp.fc_1.linear.weight', 'transformer.h.0.mlp.fc_2.linear.weight', 'transformer.h.0.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.1 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.1.attn.attn.lora_A', 'transformer.h.1.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.1.norm_1.weight', 'transformer.h.1.norm_2.weight', 'transformer.h.1.attn.attn.linear.weight', 'transformer.h.1.attn.proj.linear.weight', 'transformer.h.1.mlp.fc_1.linear.weight', 'transformer.h.1.mlp.fc_2.linear.weight', 'transformer.h.1.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.attn.lora_A', 'transformer.h.2.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.linear.weight', 'transformer.h.2.attn.proj.linear.weight', 'transformer.h.2.mlp.fc_1.linear.weight', 'transformer.h.2.mlp.fc_2.linear.weight', 'transformer.h.2.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.attn.lora_A', 'transformer.h.3.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.linear.weight', 'transformer.h.3.attn.proj.linear.weight', 'transformer.h.3.mlp.fc_1.linear.weight', 'transformer.h.3.mlp.fc_2.linear.weight', 'transformer.h.3.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.attn.lora_A', 'transformer.h.4.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.linear.weight', 'transformer.h.4.attn.proj.linear.weight', 'transformer.h.4.mlp.fc_1.linear.weight', 'transformer.h.4.mlp.fc_2.linear.weight', 'transformer.h.4.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.attn.lora_A', 'transformer.h.5.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.linear.weight', 'transformer.h.5.attn.proj.linear.weight', 'transformer.h.5.mlp.fc_1.linear.weight', 'transformer.h.5.mlp.fc_2.linear.weight', 'transformer.h.5.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.attn.lora_A', 'transformer.h.6.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.linear.weight', 'transformer.h.6.attn.proj.linear.weight', 'transformer.h.6.mlp.fc_1.linear.weight', 'transformer.h.6.mlp.fc_2.linear.weight', 'transformer.h.6.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.attn.lora_A', 'transformer.h.7.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.linear.weight', 'transformer.h.7.attn.proj.linear.weight', 'transformer.h.7.mlp.fc_1.linear.weight', 'transformer.h.7.mlp.fc_2.linear.weight', 'transformer.h.7.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.attn.lora_A', 'transformer.h.8.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.linear.weight', 'transformer.h.8.attn.proj.linear.weight', 'transformer.h.8.mlp.fc_1.linear.weight', 'transformer.h.8.mlp.fc_2.linear.weight', 'transformer.h.8.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters  than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.attn.lora_A', 'transformer.h.9.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.linear.weight', 'transformer.h.9.attn.proj.linear.weight', 'transformer.h.9.mlp.fc_1.linear.weight', 'transformer.h.9.mlp.fc_2.linear.weight', 'transformer.h.9.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.attn.lora_A', 'transformer.h.10.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.linear.weight', 'transformer.h.10.attn.proj.linear.weight', 'transformer.h.10.mlp.fc_1.linear.weight', 'transformer.h.10.mlp.fc_2.linear.weight', 'transformer.h.10.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.attn.lora_A', 'transformer.h.11.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The fhave requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.linear.weight', 'transformer.h.11.attn.proj.linear.weight', 'transformer.h.11.mlp.fc_1.linear.weight', 'transformer.h.11.mlp.fc_2.linear.weight', 'transformer.h.11.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.attn.lora_A', 'transformer.h.12.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.linear.weight', 'transformer.h.12.attn.proj.linear.weight', 'transformer.h.12.mlp.fc_1.linear.weight', 'transformer.h.12.mlp.fc_2.linear.weight', 'transformer.h.12.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.attn.lora_A', 'transformer.h.13.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transfollowing parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.linear.weight', 'transformer.h.13.attn.proj.linear.weight', 'transformer.h.13.mlp.fc_1.linear.weight', 'transformer.h.13.mlp.fc_2.linear.weight', 'transformer.h.13.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.attn.lora_A', 'transformer.h.14.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.linear.weight', 'transformer.h.14.attn.proj.linear.weight', 'transformer.h.14.mlp.fc_1.linear.weight', 'transformer.h.14.mlp.fc_2.linear.weight', 'transformer.h.14.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.attn.lora_A', 'transformer.h.15.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.linear.weight', 'transformer.h.15.attn.proj.linear.weight', 'transformer.h.15.mlp.fc_1.linear.weight', 'transformer.h.15.mlp.fc_2.linear.weight', 'transformer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linormer.h.15.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.attn.lora_A', 'transformer.h.16.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.linear.weight', 'transformer.h.16.attn.proj.linear.weight', 'transformer.h.16.mlp.fc_1.linear.weight', 'transformer.h.16.mlp.fc_2.linear.weight', 'transformer.h.16.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.attn.lora_A', 'transformer.h.17.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.linear.weight', 'transformer.h.17.attn.proj.linear.weight', 'transformer.h.17.mlp.fc_1.linear.weight', 'transformer.h.17.mlp.fc_2.linear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
ear.weight', 'transformer.h.17.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.attn.lora_A', 'transformer.h.18.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.linear.weight', 'transformer.h.18.attn.proj.linear.weight', 'transformer.h.18.mlp.fc_1.linear.weight', 'transformer.h.18.mlp.fc_2.linear.weight', 'transformer.h.18.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.attn.lora_A', 'transformer.h.19.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.linear.weight', 'transformer.h.19.attn.proj.linear.weight', 'transformer.h.19.mlp.fc_1.linear.weight', 'transformer.h.19.mlp.fc_2.linear.weight', 'transformer.h.19.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.attn.lora_A', 'transformer.h.20.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.linear.weight', 'transformer.h.20.attn.proj.linear.weight', 'transformer.h.20.mlp.fc_1.linear.weight', 'transformer.h.20.mlp.fc_2.linear.weight', 'transformer.h.20.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.attn.lora_A', 'transformer.h.21.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.linear.weight', 'transformer.h.21.attn.proj.linear.weight', 'transformer.h.21.mlp.fc_1.linear.weight', 'transformer.h.21.mlp.fc_2.linear.weight', 'transformer.h.21.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.attn.lora_A', 'transformer.h.22.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.linear.weight', 'transformer.h.22.attn.proj.linear.weight', 'transformer.h.22.mlp.fc_1.linear.weight', 'transformer.h.22.mlp.fc_2.linear.weight', 'transformer.h.22.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.attn.lora_A', 'transformer.h.23.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.linear.weight', 'transformer.h.23.attn.proj.linear.weight', 'transformer.h.23.mlp.fc_1.linear.weight', 'transformer.h.23.mlp.fc_2.linear.weight', 'transformer.h.23.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.attn.lora_A', 'transformer.h.24.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.linear.weight', 'transformer.h.24.attn.proj.linear.weight', 'transformer.h.24.mlp.fc_1.linear.weight', 'transformer.h.24.mlp.fc_2.linear.weight', 'transformer.h.24.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.attn.lora_A', 'transformer.h.25.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.linear.weight', 'transformer.h.25.attn.proj.linear.weight', 'transformer.h.25.mlp.fc_1.linear.weight', 'transformer.h.25.mlp.fc_2.linear.weight', 'transformer.h.25.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.attn.lora_A', 'transformer.h.26.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.linear.weight', 'transformer.h.26.attn.proj.linear.weight', 'transformer.h.26.mlp.fc_1.linear.weight', 'transformer.h.26.mlp.fc_2.linear.weight', 'transformer.h.26.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.attn.lora_A', 'transformer.h.27.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.linear.weight', 'transformer.h.27.attn.proj.linear.weight', 'transformer.h.27.mlp.fc_1.linear.weight', 'transformer.h.27.mlp.fc_2.linear.weight', 'transformer.h.27.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.attn.lora_A', 'transformer.h.28.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.linear.weight', 'transformer.h.28.attn.proj.linear.weight', 'transformer.h.28.mlp.fc_1.linear.weight', 'transformer.h.28.mlp.fc_2.linear.weight', 'transformer.h.28.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.attn.lora_A', 'transformer.h.29.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.linear.weight', 'transformer.h.29.attn.proj.linear.weight', 'transformer.h.29.mlp.fc_1.linear.weight', 'transformer.h.29.mlp.fc_2.linear.weight', 'transformer.h.29.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.attn.lora_A', 'transformer.h.30.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.linear.weight', 'transformer.h.30.attn.proj.linear.weight', 'transformer.h.30.mlp.fc_1.linear.weight', 'transformer.h.30.mlp.fc_2.linear.weight', 'transformer.h.30.mlp.proj.linear.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202514432 numel instead of 131072 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.attn.lora_A', 'transformer.h.31.attn.attn.lora_B']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.linear.weight', 'transformer.h.31.attn.proj.linear.weight', 'transformer.h.31.mlp.fc_1.linear.weight', 'transformer.h.31.mlp.fc_2.linear.weight', 'transformer.h.31.mlp.proj.linear.weight']
  warnings.warn(msg)
x1000c2s1b0n1:3320354:3320354 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.186<0>
x1000c2s1b0n1:3320354:3320354 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s1b0n1:3320354:3320354 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
x1000c2s1b0n1:3320355:3320355 [1] NCCL INFO cudaDriverVersion 12020
x1000c2s7b0n0:2841744:2841744 [1] NCCL INFO cudaDriverVersion 12020
x1000c2s1b0n1:3320357:3320357 [3] NCCL INFO cudaDriverVersion 12020
x1000c2s1b0n1:3320356:3320356 [2] NCCL INFO cudaDriverVersion 12020
x1000c2s7b0n0:2841744:2841744 [1] NCCL INFO Bootstrap : Using hsn0:10.150.0.104<0>
x1000c2s7b0n0:2841744:2841744 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s7b0n0:2841746:2841746 [3] NCCL INFO cudaDriverVersion 12020
x1000c2s7b0n0:2841745:2841745 [2] NCCL INFO cudaDriverVersion 12020
x1000c2s7b0n0:2841743:2841743 [0] NCCL INFO cudaDriverVersion 12020
x1000c2s1b0n1:3320355:3320355 [1] NCCL INFO Bootstrap : Using hsn0:10.150.0.186<0>
x1000c2s1b0n1:3320357:3320357 [3] NCCL INFO Bootstrap : Using hsn0:10.150.0.186<0>
x1000c2s1b0n1:3320356:3320356 [2] NCCL INFO Bootstrap : Using hsn0:10.150.0.186<0>
x1000c2s1b0n1:3320357:3320357 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s1b0n1:3320356:3320356 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s1b0n1:3320355:3320355 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s7b0n0:2841746:2841746 [3] NCCL INFO Bootstrap : Using hsn0:10.150.0.104<0>
x1000c2s7b0n0:2841745:2841745 [2] NCCL INFO Bootstrap : Using hsn0:10.150.0.104<0>
x1000c2s7b0n0:2841743:2841743 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.104<0>
x1000c2s7b0n0:2841746:2841746 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s7b0n0:2841745:2841745 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s7b0n0:2841743:2841743 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.104<0> [1]hsn1:10.150.0.197<0> [2]bond0:10.168.0.9<0>
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Using non-device net plugin version 0
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Using network Socket
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.104<0> [1]hsn1:10.150.0.197<0> [2]bond0:10.168.0.9<0>
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.104<0> [1]hsn1:10.150.0.197<0> [2]bond0:10.168.0.9<0>
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.104<0> [1]hsn1:10.150.0.197<0> [2]bond0:10.168.0.9<0>
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Using non-device net plugin version 0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Using network Socket
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Using non-device net plugin version 0
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Using network Socket
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Using non-device net plugin version 0
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Using network Socket
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.186<0> [1]hsn1:10.150.0.3<0> [2]bond0:10.168.0.28<0>
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Using non-device net plugin version 0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Using network Socket
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.186<0> [1]hsn1:10.150.0.3<0> [2]bond0:10.168.0.28<0>
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.186<0> [1]hsn1:10.150.0.3<0> [2]bond0:10.168.0.28<0>
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.186<0> [1]hsn1:10.150.0.3<0> [2]bond0:10.168.0.28<0>
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Using non-device net plugin version 0
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Using network Socket
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Using non-device net plugin version 0
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Using network Socket
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Using non-device net plugin version 0
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Using network Socket
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO comm 0x560206f6cfe0 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x486be8041c52f630 - Init START
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO comm 0x55c55abfe710 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x486be8041c52f630 - Init START
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO comm 0x5635d4700fb0 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x486be8041c52f630 - Init START
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO comm 0x55ce54c947d0 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x486be8041c52f630 - Init START
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO comm 0x557f6d9a0910 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x486be8041c52f630 - Init START
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO comm 0x55efe00e7d80 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x486be8041c52f630 - Init START
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO comm 0x558d47880b30 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x486be8041c52f630 - Init START
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO comm 0x5640e159d9b0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x486be8041c52f630 - Init START
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO comm 0x55ce54c947d0 rank 3 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO P2P Chunksize set to 131072
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO comm 0x55c55abfe710 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 00/04 :    0   3   6   5   4   7   2   1
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 01/04 :    0   5   7   6   4   1   3   2
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 02/04 :    0   3   6   5   4   7   2   1
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 03/04 :    0   5   7   6   4   1   3   2
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO P2P Chunksize set to 131072
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO comm 0x560206f6cfe0 rank 1 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO P2P Chunksize set to 131072
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO comm 0x5635d4700fb0 rank 2 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO P2P Chunksize set to 131072
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO comm 0x557f6d9a0910 rank 6 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO P2P Chunksize set to 131072
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO comm 0x55efe00e7d80 rank 5 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO P2P Chunksize set to 131072
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO comm 0x5640e159d9b0 rank 7 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO P2P Chunksize set to 131072
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO comm 0x558d47880b30 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/-1/-1->4->0 [2] 5/0/-1->4->-1 [3] 5/0/-1->4->-1
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO P2P Chunksize set to 131072
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 00/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 01/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 00/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 03/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 02/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 03/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 02/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 01/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 01/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 01/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 01/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 03/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 03/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 00/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 02/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 00/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 02/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 03/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 03/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 00/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 02/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 00/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 02/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Connected all rings
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 00/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Connected all rings
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Connected all rings
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Connected all rings
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 01/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Connected all rings
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 00/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Connected all rings
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Connected all rings
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Connected all rings
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 01/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 02/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 03/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 02/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 03/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 00/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 00/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 01/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 01/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 00/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 02/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 02/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 01/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 03/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 03/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 02/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 02/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 00/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 03/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Channel 02/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 01/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Channel 03/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Channel 03/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 03/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 00/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Channel 02/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 01/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 01/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Channel 03/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Channel 03/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO Connected all trees
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO Connected all trees
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO Connected all trees
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO Connected all trees
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO Connected all trees
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO Connected all trees
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO Connected all trees
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO Connected all trees
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s1b0n1:3320357:3320939 [3] NCCL INFO comm 0x55ce54c947d0 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s1b0n1:3320356:3320938 [2] NCCL INFO comm 0x5635d4700fb0 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s1b0n1:3320354:3320937 [0] NCCL INFO comm 0x55c55abfe710 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s1b0n1:3320355:3320940 [1] NCCL INFO comm 0x560206f6cfe0 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s7b0n0:2841746:2842323 [3] NCCL INFO comm 0x5640e159d9b0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s7b0n0:2841744:2842322 [1] NCCL INFO comm 0x55efe00e7d80 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s7b0n0:2841743:2842325 [0] NCCL INFO comm 0x558d47880b30 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x486be8041c52f630 - Init COMPLETE
x1000c2s7b0n0:2841745:2842324 [2] NCCL INFO comm 0x557f6d9a0910 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x486be8041c52f630 - Init COMPLETE
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[x1000c2s1b0n1:3320340] 8 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
Epoch 1 | iter 1 step 1 | loss train: 1.518, val: n/a | iter time: 11770.82 ms (step)
Epoch 1 | iter 1 step 1 | loss train: 1.425, val: n/a | iter time: 11807.42 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.485, val: n/a | iter time: 10791.44 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.491, val: n/a | iter time: 10735.30 ms (step)
Epoch 2 | iter 3 step 3 | loss train: 1.479, val: n/a | iter time: 10958.21 ms (step)
Epoch 2 | iter 3 step 3 | loss train: 1.441, val: n/a | iter time: 10955.97 ms (step)
Training time: 40.53s
Memory used: 26.02 GB
Saving LoRA weights to '/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first/final/lit_model.pth.lora'
Training time: 40.55s
Memory used: 26.04 GB
Saving LoRA weights to '/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first/final/lit_model.pth.lora'
x1000c2s7b0n0:2841743:2842333 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c2s7b0n0:2841744:2842327 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c2s7b0n0:2841745:2842329 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c2s1b0n1:3320356:3320946 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c2s7b0n0:2841746:2842328 [3] NCCL INFO [Service thread] Connection closed by localRank 3
x1000c2s1b0n1:3320355:3320949 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c2s1b0n1:3320357:3320947 [3] NCCL INFO [Service thread] Connection closed by localRank 3
{'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first/final'),
 'precision': None,
 'pretrained_checkpoint_dir': None}
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / "lit_model.pth"), mmap=True)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  lora_checkpoint = torch.load(str(lora_path), mmap=True)
x1000c2s1b0n1:3320356:3324386 [2] NCCL INFO comm 0x5635d4700fb0 rank 2 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c2s1b0n1:3320357:3324388 [3] NCCL INFO comm 0x55ce54c947d0 rank 3 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c2s1b0n1:3320355:3324387 [1] NCCL INFO comm 0x560206f6cfe0 rank 1 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c2s7b0n0:2841746:2845781 [3] NCCL INFO comm 0x5640e159d9b0 rank 7 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c2s7b0n0:2841744:2845780 [1] NCCL INFO comm 0x55efe00e7d80 rank 5 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c2s7b0n0:2841745:2845779 [2] NCCL INFO comm 0x557f6d9a0910 rank 6 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c2s7b0n0:2841743:2845778 [0] NCCL INFO comm 0x558d47880b30 rank 4 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
Saved merged weights to '/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/lora256/first/final/lit_model.pth'
x1000c2s1b0n1:3320354:3320952 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c2s1b0n1:3320354:3324414 [0] NCCL INFO comm 0x55c55abfe710 rank 0 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
