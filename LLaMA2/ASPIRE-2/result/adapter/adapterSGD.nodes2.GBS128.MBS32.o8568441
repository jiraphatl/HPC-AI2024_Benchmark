-bash: module: line 1: syntax error: unexpected end of file
-bash: error importing function definition for `module'
/bin/sh: module: line 1: syntax error: unexpected end of file
/bin/sh: error importing function definition for `module'
/bin/sh: module: line 1: syntax error: unexpected end of file
/bin/sh: error importing function definition for `module'
/bin/sh: module: line 1: syntax error: unexpected end of file
/bin/sh: error importing function definition for `module'
/bin/bash: module: line 1: syntax error: unexpected end of file
/bin/bash: error importing function definition for `module'
Sun Nov  3 13:07:10 +08 2024
CONDA_SHLVL=1
PBS_ENVIRONMENT=PBS_BATCH
LD_LIBRARY_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64:/app/apps/openmpi/4.1.2-hpe/lib:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/lib:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/lib:/app/apps/local/lib64:/app/apps/local/lib
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:
CONDA_EXE=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin/conda
PBS_O_LANG=en_US.UTF-8
PRGENVMODULES=PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pgi:PrrgEnv-amd
max_steps=20
SSH_CONNECTION=10.103.62.26 54330 10.103.153.100 22
LANG=en_US.UTF-8
TZ=Asia/Singapore
HISTCONTROL=ignoredups
DISPLAY=localhost:10.0
HOSTNAME=x1000c1s2b0n1
OLDPWD=/home/users/industry/ai-hpc/apacsc41/run/llama-run
PBS_O_HOME=/home/users/industry/ai-hpc/apacsc41
PBS_JOBID=8568441.pbs101
ENVIRONMENT=BATCH
global_batch_size=128
PBS_JOBNAME=adapterSGD.nodes2.GBS128.MBS32
FPATH=:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
NCPUS=64
CONDA_PREFIX=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3
PBS_O_PATH=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/condabin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/home/users/industry/ai-hpc/apacsc41/.local/bin:/home/users/industry/ai-hpc/apacsc41/bin:/opt/cray/pe/pals/1.1.6/bin:/opt/cray/pe/craype/2.7.15/bin:/opt/cray/pe/cce/13.0.2/binutils/x86_64/x86_64-pc-linux-gnu/bin:/opt/cray/pe/cce/13.0.2/binutils/cross/x86_64-aarch64/bin:/opt/cray/pe/cce/13.0.2/utils/x86_64/bin:/opt/cray/pe/cce/13.0.2/bin:/opt/cray/pe/perftools/22.04.0/bin:/opt/cray/pe/papi/6.0.0.14/bin:/opt/cray/libfabric/1.11.0.4.125/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/cray/pe/bin
S_COLORS=auto
C3_RSH=ssh -oConnectTimeout=10 -oForwardX11=no
_CE_M=
XDG_SESSION_ID=86966
PE_OPENMPI_BASEDIR=/app/apps/openmpi/4.1.2-hpe
PBS_O_WORKDIR=/home/users/industry/ai-hpc/apacsc41/run/llama-run/new
USER=apacsc41
MODULE_VERSION=3.2.11.6
TOOLMODULES=apprentice:apprentice2:atp:ccdb:cdt:chapel:cray-cti:cray-lgdb:craypat:craypkg-gen:cray-R:cray-snplauncher:ddt:gdb:gdb4hpc:iobuf:papi:perftools:perftools-lite:python:stat:totalview:xt-craypat:xt-lgdb:xt-papi:xt-totalview
micro_batch_size=32
PBS_NODEFILE=/var/spool/pbs/aux/8568441.pbs101
KSH_AUTOLOAD=1
PE_FORTRAN_PKGCONFIG_LIBS=ompi-fort
PBS_TASKNUM=1
PWD=/home/users/industry/ai-hpc/apacsc41
TARGETMODULES=craype-accel-amd-gfx908:craype-accel-amd-gfx90a:craype-accel-host:craype-accel-nvidia70:craype-accel-nvidia80:craype-network-none:craype-network-ofi:craype-network-ucx:craype-x86-milan:craype-x86-milan-x:craype-x86-rome:craype-x86-trento
HOME=/home/users/industry/ai-hpc/apacsc41
CONDA_PYTHON_EXE=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin/python
PELOCAL_PRGENV=true
SSH_CLIENT=10.103.62.26 54330 22
CPATH=/app/apps/openmpi/4.1.2-hpe/include
PBS_MOMPORT=15003
OPENMPI_VERSION=4.1.2-hpe
XDG_DATA_DIRS=/home/users/industry/ai-hpc/apacsc41/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
PE_CXX_PKGCONFIG_LIBS=ompi-cxx
_CE_CONDA=
GSETTINGS_SCHEMA_DIR=/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/share/glib-2.0/schemas
PBS_JOBCOOKIE=139BBC1302249A142F405F6C763F2603
PBS_O_SHELL=/bin/bash
TMPDIR=/var/tmp/pbs.8568441.pbs101
LIBRARY_PATH=/app/apps/openmpi/4.1.2-hpe/lib
PE_PKGCONFIG_LIBS=ompi
USERMODULES=acml:alps:apprentice:apprentice2:atp:blcr:cce:chapel:cray-ccdb:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-lgdb:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-compat:cray-netcdf:cray-netcdf-hdf5parallel:cray-openshmemx:cray-parallel-netcdf:craypat:craype:cray-petsc:cray-petsc-complex:craypkg-gen:cray-R:cray-shmem:cray-snplauncher:cray-tpsl:cray-trilinos:cudatoolkit:ddt:fftw:ga:gcc:hdf5:hdf5-parallel:intel:iobuf:java:lgdb:libfast:libsci_acc:mpich1:netcdf:netcdf-hdf5parallel:netcdf-nofsync:netcdf-nofsync-hdf5parallel:ntk:nvidia:onesided:papi:parallel-netcdf:pathscale:perftools:perftools-lite:petsc:petsc-complex:pgi:pmi:PrgEnv-amd:PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pathscale:PrgEnv-pgi:python:rocm-compiler:stat:totalview:tpsl:trilinos:xt-asyncpe:xt-craypat:xt-lgdb:xt-libsci:xt-mpich2:xt-mpt:xt-papi:xt-shmem:xt-totalview
LOADEDMODULES=openmpi/4.1.2-hpe:libfabric/1.11.0.4.125
LIBRARYMODULES=acml:alps:cray-dwarf:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-abi:cray-netcdf:cray-netcdf-hdf5parallel:cray-parallel-netcdf:cray-petsc:cray-petsc-complex:cray-shmem:cray-tpsl:cray-trilinos:cudatoolkit:fftw:ga:hdf5:hdf5-parallel:iobuf:libfast:netcdf:netcdf-hdf5parallel:ntk:onesided:papi:petsc:petsc-complex:pmi:tpsl:trilinos:xt-libsci:xt-mpich2:xt-mpt:xt-papi
CONDA_PROMPT_MODIFIER=(base) 
SSH_TTY=/dev/pts/4
MAIL=/var/spool/mail/apacsc41
MOTDLOCK=1
CRAY_SITE_LIST_DIR=/etc/cray-pe.d/cray-modules
SHELL=/bin/bash
TERM=xterm
walltime=7201
USE_PCM_DB=2
CUDA_DEVICE_ORDER=PCI_BUS_ID
CUDA_VISIBLE_DEVICES=GPU-b9c460cf-379a-fa10-a3cf-2c1c43e7aa66,GPU-fa399eca-47fe-1169-9faa-2700024ea798,GPU-f7795f7c-a2fb-5ccc-831e-5e86b6ab61f2,GPU-ddc17379-18f4-843e-46eb-ca4ed4028beb
SHLVL=4
LANGUAGE=en_US.UTF-8
PBS_O_HOST=asp2a-login-nscc01.head.cm.asp2a.nscc.sg
PBS_O_SYSTEM=Linux
MANPATH=/opt/cray/libfabric/1.11.0.4.125/share/man:/app/apps/openmpi/4.1.2-hpe/share/man:/opt/cray/pe/modules/3.2.11.6/share/man:/opt/cray/pe/modules/3.2.11.6/share/man::/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man
OSCAR_HOME=/opt/oscar
PBS_O_LOGNAME=apacsc41
PBS_NODENUM=0
GDK_BACKEND=x11
MODULEPATH=/opt/cray/pe/modulefiles:/opt/cray/modulefiles:/opt/modulefiles:/opt/cray/pe/craype-targets/default/modulefiles:/app/apps/modulefiles:/app/libs/modulefiles
PBS_JOBDIR=/home/users/industry/ai-hpc/apacsc41
LOGNAME=apacsc41
DBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-ZBabF8DDp8,guid=e43b8d094c742876a6668d636726fa2f
XDG_RUNTIME_DIR=/run/user/12353
BASH_FUNC_module%%=() {  eval `/opt/cray/pe/modules/3.2.11.6/bin/modulecmd bash $*`
nodes=2
MODULE_VERSION_STACK=3.2.11.6
PE_C_PKGCONFIG_LIBS=ompi-c
OPENMPI_DIR=/app/apps/openmpi/4.1.2-hpe
PATH=/opt/cray/libfabric/1.11.0.4.125/bin:/app/apps/openmpi/4.1.2-hpe/bin:/app/apps/openmpi/4.1.2-hpe/include:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/bin:/home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/condabin:/home/users/industry/ai-hpc/apacsc41/proj/app/libunwind/bin:/home/users/industry/ai-hpc/apacsc41/.local/bin:/home/users/industry/ai-hpc/apacsc41/bin:/opt/cray/pe/cce/13.0.2/binutils/cross/x86_64-aarch64/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/pbs/bin:/sbin:/bin
_LMFILES_=/app/apps/modulefiles/openmpi/4.1.2-hpe:/opt/cray/modulefiles/libfabric/1.11.0.4.125
PBS_QUEUE=g4
MODULESHOME=/opt/cray/pe/modules/3.2.11.6
PKG_CONFIG_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64/pkgconfig:/app/apps/openmpi/4.1.2-hpe/lib/pkgconfig
CONDA_DEFAULT_ENV=base
HISTSIZE=1000
XML_CATALOG_FILES=file:///home/users/industry/ai-hpc/apacsc41/proj/app/miniconda3/etc/xml/catalog file:///etc/xml/catalog
OPENMPI_POST_LINK_OPTS=-L/app/apps/openmpi/4.1.2-hpe/lib -Wl,-rpath=/app/apps/openmpi/4.1.2-hpe/lib
PBS_O_MAIL=/var/spool/mail/apacsc41
LESSOPEN=||/usr/bin/lesspipe.sh %s
OMP_NUM_THREADS=64
LC_TIME=en_US.UTF-8
BASH_FUNC_module%%=() {  eval `/opt/cray/pe/modules/3.2.11.6/bin/modulecmd bash $*`
}
_=/usr/bin/env
x1000c1s2b0n1.hostmgmt2000.cm.asp2a.nscc.sg
x1000c1s4b0n1.hostmgmt2000.cm.asp2a.nscc.sg
mpirun -wdir /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama -output-filename /home/users/industry/ai-hpc/apacsc41/run/output/adapter/adapterSGD.nodes2.GBS128.MBS32.8568441.pbs101 -map-by ppr:4:node -oversubscribe -report-bindings -x NCCL_DEBUG=INFO -x NCCL_NET_GDR_LEVEL=0 -x NCCL_IB_DISABLE=1 -mca pml ^ucx /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/bin/python /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py finetune_adapter /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf --out_dir /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD --data JSON --data.json_path /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024 --config /home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/adaptive.yaml --eval.final_validation=true --train.epochs=1 --devices=4 --num_nodes=2 --train.max_steps=20 --train.global_batch_size=128 --train.micro_batch_size=32
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x147e6df4a480>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x148893f8a120>,
              ignore_index=-100,
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14d3aed47dd0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14f7739b8950>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x15251c28c110>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14ef3e7056a0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x147e952251f0>,
              ignore_index=-100,
              seed=42,
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x152610c9e570>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': {'class_path': 'torch.optim.SGD'},
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/out/finetune/new/adapter_SGD'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=5,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c1s2b0n1:1326658] MCW rank 3 is not bound (or bound to all available processors)
[x1000c1s2b0n1:1326655] MCW rank 0 is not bound (or bound to all available processors)
[x1000c1s2b0n1:1326657] MCW rank 2 is not bound (or bound to all available processors)
[x1000c1s2b0n1:1326656] MCW rank 1 is not bound (or bound to all available processors)
[x1000c1s4b0n1:1161783] MCW rank 7 is not bound (or bound to all available processors)
[x1000c1s4b0n1:1161782] MCW rank 6 is not bound (or bound to all available processors)
[x1000c1s4b0n1:1161781] MCW rank 5 is not bound (or bound to all available processors)
[x1000c1s4b0n1:1161780] MCW rank 4 is not bound (or bound to all available processors)
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           x1000c1s2b0n1
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
All GPUs are fully connected via NVLink.
All GPUs are fully connected via NVLink.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Number of trainable parameters: 1,229,760
Number of non-trainable parameters: 6,738,415,616
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
Seed set to 1337
Seed set to 1337
Seed set to 1337
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
Number of trainable parameters: 1,229,760
Number of non-trainable parameters: 6,738,415,616
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wra/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.2 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.2.attn.gating_factor', 'transformer.h.2.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.2.norm_1.weight', 'transformer.h.2.norm_2.weight', 'transformer.h.2.attn.attn.weight', 'transformer.h.2.attn.proj.weight', 'transformer.h.2.mlp.fc_1.weight', 'transformer.h.2.mlp.fc_2.weight', 'transformer.h.2.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.3 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.3.attn.gating_factor', 'transformer.h.3.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.3.norm_1.weight', 'transformer.h.3.norm_2.weight', 'transformer.h.3.attn.attn.weight', 'transformer.h.3.attn.proj.weight', 'transformer.h.3.mlp.fc_1.weight', 'transformer.h.3.mlp.fc_2.weight', 'transformer.h.3.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.4 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.4.attn.gating_factor', 'transformer.h.4.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.4.norm_1.weight', 'transformer.h.4.norm_2.weight', 'transformer.h.4.attn.attn.weight', 'transformer.h.4.attn.proj.weight', 'transformer.h.4.mlp.fc_1.weight', 'transformer.h.4.mlp.fc_2.weight', 'transformer.h.4.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If pop the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
ssible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight',/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.5 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.5.attn.gating_factor', 'transformer.h.5.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.5.norm_1.weight', 'transformer.h.5.norm_2.weight', 'transformer.h.5.attn.attn.weight', 'transformer.h.5.attn.proj.weight', 'transformer.h.5.mlp.fc_1.weight', 'transformer.h.5.mlp.fc_2.weight', 'transformer.h.5.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.6 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.6.attn.gating_factor', 'transformer.h.6.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.6.norm_1.weight', 'transformer.h.6.norm_2.weight', 'transformer.h.6.attn.attn.weight', 'transformer.h.6.attn.proj.weight', 'transformer.h.6.mlp.fc_1.weight', 'transformer.h.6.mlp.fc_2.weight', 'transformer.h.6.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.7 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.7.attn.gating_factor', 'transformer.h.7.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.7.norm_1.weight', 'transformer.h.7.norm_2.weight', 'transformer.h.7.attn.attn.weight', 'transformer.h.7.attn.proj.weight', 'transformer.h.7.mlp.fc_1.weight', 'transformer.h.7.mlp.fc_2.weight', 'transformer.h.7.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.8 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.8.attn.gating_factor', 'transformer.h.8.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.8.norm_1.weight', 'transformer.h.8.norm_2.weight', 'transformer.h.8.attn.attn.weight', 'transformer.h.8.attn.proj.weight', 'transformer.h.8.mlp.fc_1.weight', 'transformer.h.8.mlp.fc_2.weight', 'transformer.h.8.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.9 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wra/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.mlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before shard 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transp the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.9.attn.gating_factor', 'transformer.h.9.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.9.norm_1.weight', 'transformer.h.9.norm_2.weight', 'transformer.h.9.attn.attn.weight', 'transformer.h.9.attn.proj.weight', 'transformer.h.9.mlp.fc_1.weight', 'transformer.h.9.mlp.fc_2.weight', 'transformer.h.9.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.10 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.10.attn.gating_factor', 'transformer.h.10.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.10.norm_1.weight', 'transformer.h.10.norm_2.weight', 'transformer.h.10.attn.attn.weight', 'transformer.h.10.attn.proj.weight', 'transformer.h.10.mlp.fc_1.weight', 'transformer.h.10.mlp.fc_2.weight', 'transformer.h.10.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.11 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.11.attn.gating_factor', 'transformer.h.11.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.11.norm_1.weight', 'transformer.h.11.norm_2.weight', 'transformer.h.11.attn.attn.weight', 'transformer.h.11.attn.proj.weight', 'transformer.h.11.mlp.fc_1.weight', 'transformer.h.11.mlp.fc_2.weight', 'transformer.h.11.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.12 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.12.attn.gating_factor', 'transformer.h.12.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.12.norm_1.weight', 'transformer.h.12.norm_2.weight', 'transformer.h.12.attn.attn.weight', 'transformer.h.12.attn.proj.weight', 'transformer.h.12.mlp.fc_1.weight', 'transformer.h.12.mlp.fc_2.weight', 'transformer.h.12.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.13 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.13.attn.gating_factor', 'transformer.h.13.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.13.norm_1.weight', 'transformer.h.13.norm_2.weight', 'transformer.h.13.attn.attn.weight', 'transformer.h.13.attn.proj.weight', 'transformer.h.13.ming such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have reqlp.fc_1.weight', 'transformer.h.13.mlp.fc_2.weight', 'transformer.h.13.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.14 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.14.attn.gating_factor', 'transformer.h.14.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.14.norm_1.weight', 'transformer.h.14.norm_2.weight', 'transformer.h.14.attn.attn.weight', 'transformer.h.14.attn.proj.weight', 'transformer.h.14.mlp.fc_1.weight', 'transformer.h.14.mlp.fc_2.weight', 'transformer.h.14.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.15 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.15.attn.gating_factor', 'transformer.h.15.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.15.norm_1.weight', 'transformer.h.15.norm_2.weight', 'transformer.h.15.attn.attn.weight', 'transformer.h.15.attn.proj.weight', 'transformer.h.15.mlp.fc_1.weight', 'transformer.h.15.mlp.fc_2.weight', 'transformer.h.15.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.16 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.16.attn.gating_factor', 'transformer.h.16.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.16.norm_1.weight', 'transformer.h.16.norm_2.weight', 'transformer.h.16.attn.attn.weight', 'transformer.h.16.attn.proj.weight', 'transformer.h.16.mlp.fc_1.weight', 'transformer.h.16.mlp.fc_2.weight', 'transformer.h.16.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.17 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.17.attn.gating_factor', 'transformer.h.17.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.17.norm_1.weight', 'transformer.h.17.norm_2.weight', 'transformer.h.17.attn.attn.weight', 'transformer.h.17.attn.proj.weight', 'transformer.h.17.mlp.fc_1.weight', 'transformer.h.17.mlp.fc_2.weight', 'transformer.h.17.mlp.proj.weight']
  warnings.warn(msg)
former.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
uires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-pa/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
ckages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.18 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.18.attn.gating_factor', 'transformer.h.18.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.18.norm_1.weight', 'transformer.h.18.norm_2.weight', 'transformer.h.18.attn.attn.weight', 'transformer.h.18.attn.proj.weight', 'transformer.h.18.mlp.fc_1.weight', 'transformer.h.18.mlp.fc_2.weight', 'transformer.h.18.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.19 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.19.attn.gating_factor', 'transformer.h.19.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.19.norm_1.weight', 'transformer.h.19.norm_2.weight', 'transformer.h.19.attn.attn.weight', 'transformer.h.19.attn.proj.weight', 'transformer.h.19.mlp.fc_1.weight', 'transformer.h.19.mlp.fc_2.weight', 'transformer.h.19.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.20 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.20.attn.gating_factor', 'transformer.h.20.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.20.norm_1.weight', 'transformer.h.20.norm_2.weight', 'transformer.h.20.attn.attn.weight', 'transformer.h.20.attn.proj.weight', 'transformer.h.20.mlp.fc_1.weight', 'transformer.h.20.mlp.fc_2.weight', 'transformer.h.20.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.21 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.21.attn.gating_factor', 'transformer.h.21.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.21.norm_1.weight', 'transformer.h.21.norm_2.weight', 'transformer.h.21.attn.attn.weight', 'transformer.h.21.attn.proj.weight', 'transformer.h.21.mlp.fc_1.weight', 'transformer.h.21.mlp.fc_2.weight', 'transformer.h.21.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.22 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.22.attn.gating_factor', 'transformer.h.22.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.22.norm_1.weight', 'transformer.h.22.norm_2.weight', 'transformer.h.22.attn.attn.weight', 'transformer.h.22.attn.proj.weight', 'transformer.h.22.mlp.fc_1.weight', 'transformer.h.22.mlp.fc_2.weight', 'transformer.h.22.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.23 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.23.attn.gating_factor', 'transformer.h.23.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.23.norm_1.weight', 'transformer.h.23.norm_2.weight', 'transformer.h.23.attn.attn.weight', 'transformer.h.23.attn.proj.weight', 'transformer.h.23.mlp.fc_1.weight', 'transformer.h.23.mlp.fc_2.weight', 'transformer.h.23.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.24 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.24.attn.gating_factor', 'transformer.h.24.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.24.norm_1.weight', 'transformer.h.24.norm_2.weight', 'transformer.h.24.attn.attn.weight', 'transformer.h.24.attn.proj.weight', 'transformer.h.24.mlp.fc_1.weight', 'transformer.h.24.mlp.fc_2.weight', 'transformer.h.24.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.25 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.25.attn.gating_factor', 'transformer.h.25.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.25.norm_1.weight', 'transformer.h.25.norm_2.weight', 'transformer.h.25.attn.attn.weight', 'transformer.h.25.attn.proj.weight', 'transformer.h.25.mlp.fc_1.weight', 'transformer.h.25.mlp.fc_2.weight', 'transformer.h.25.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.26 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.26.attn.gating_factor', 'transformer.h.26.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.26.norm_1.weight', 'transformer.h.26.norm_2.weight', 'transformer.h.26.attn.attn.weight', 'transformer.h.26.attn.proj.weight', 'transformer.h.26.mlp.fc_1.weight', 'transformer.h.26.mlp.fc_2.weight', 'transformer.h.26.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.27 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.27.attn.gating_factor', 'transformer.h.27.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.27.norm_1.weight', 'transformer.h.27.norm_2.weight', 'transformer.h.27.attn.attn.weight', 'transformer.h.27.attn.proj.weight', 'transformer.h.27.mlp.fc_1.weight', 'transformer.h.27.mlp.fc_2.weight', 'transformer.h.27.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.28 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.28.attn.gating_factor', 'transformer.h.28.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.28.norm_1.weight', 'transformer.h.28.norm_2.weight', 'transformer.h.28.attn.attn.weight', 'transformer.h.28.attn.proj.weight', 'transformer.h.28.mlp.fc_1.weight', 'transformer.h.28.mlp.fc_2.weight', 'transformer.h.28.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.29 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.29.attn.gating_factor', 'transformer.h.29.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.29.norm_1.weight', 'transformer.h.29.norm_2.weight', 'transformer.h.29.attn.attn.weight', 'transformer.h.29.attn.proj.weight', 'transformer.h.29.mlp.fc_1.weight', 'transformer.h.29.mlp.fc_2.weight', 'transformer.h.29.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.30 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.30.attn.gating_factor', 'transformer.h.30.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.30.norm_1.weight', 'transformer.h.30.norm_2.weight', 'transformer.h.30.attn.attn.weight', 'transformer.h.30.attn.proj.weight', 'transformer.h.30.mlp.fc_1.weight', 'transformer.h.30.mlp.fc_2.weight', 'transformer.h.30.mlp.proj.weight']
  warnings.warn(msg)
/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py:175: UserWarning: transformer.h.31 has both parameters with requires_grad=True and False. We do not recommend wrapping such modules since the gradient memory usage will be higher than expected (202424352 numel instead of 40992 numel before sharding via reduce-scatter). If possible, wrap the frozen parameters with FSDP separately.
The following parameters have requires_grad=True:
['transformer.h.31.attn.gating_factor', 'transformer.h.31.attn.adapter_wte.weight']
The following parameters have requires_grad=False:
['transformer.h.31.norm_1.weight', 'transformer.h.31.norm_2.weight', 'transformer.h.31.attn.attn.weight', 'transformer.h.31.attn.proj.weight', 'transformer.h.31.mlp.fc_1.weight', 'transformer.h.31.mlp.fc_2.weight', 'transformer.h.31.mlp.proj.weight']
  warnings.warn(msg)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank1]:     main()
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank1]:     CLI(parser_data)
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank1]:     return _run_component(component, init.get(subcommand))
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank1]:     return component(**cfg)
[rank1]:            ^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank1]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank1]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank1]:     return to_run(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank1]:     return to_run(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank1]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank1]:     optimizer["init_args"].update(kwargs)
[rank1]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank1]: KeyError: 'init_args'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank2]:     main()
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank2]:     CLI(parser_data)
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank2]:     return _run_component(component, init.get(subcommand))
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank2]:     return component(**cfg)
[rank2]:            ^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank2]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank2]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank2]:     return to_run(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank2]:     return to_run(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank2]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank2]:     optimizer["init_args"].update(kwargs)
[rank2]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank2]: KeyError: 'init_args'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank3]:     main()
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank3]:     CLI(parser_data)
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank3]:     return _run_component(component, init.get(subcommand))
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank3]:     return component(**cfg)
[rank3]:            ^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank3]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank3]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank3]:     return to_run(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank3]:     return to_run(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank3]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank3]:     optimizer["init_args"].update(kwargs)
[rank3]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank3]: KeyError: 'init_args'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank7]:     main()
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank7]:     CLI(parser_data)
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank7]:     return _run_component(component, init.get(subcommand))
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank7]:     return component(**cfg)
[rank7]:            ^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank7]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank7]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank7]:     return to_run(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank7]:     return to_run(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank7]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank7]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank7]:     optimizer["init_args"].update(kwargs)
[rank7]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank7]: KeyError: 'init_args'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank6]:     main()
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank6]:     CLI(parser_data)
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank6]:     return _run_component(component, init.get(subcommand))
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank6]:     return component(**cfg)
[rank6]:            ^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank6]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank6]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank6]:     return to_run(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank6]:     return to_run(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank6]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank6]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank6]:     optimizer["init_args"].update(kwargs)
[rank6]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank6]: KeyError: 'init_args'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank5]:     main()
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank5]:     CLI(parser_data)
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank5]:     return _run_component(component, init.get(subcommand))
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank5]:     return component(**cfg)
[rank5]:            ^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank5]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank5]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank5]:     return to_run(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank5]:     return to_run(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank5]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank5]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank5]:     optimizer["init_args"].update(kwargs)
[rank5]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank5]: KeyError: 'init_args'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank0]:     main()
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank0]:     CLI(parser_data)
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank0]:     return _run_component(component, init.get(subcommand))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank0]:     return component(**cfg)
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank0]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank0]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank0]:     return to_run(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank0]:     return to_run(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank0]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank0]:     optimizer["init_args"].update(kwargs)
[rank0]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank0]: KeyError: 'init_args'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 75, in <module>
[rank4]:     main()
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt/litgpt/__main__.py", line 71, in main
[rank4]:     CLI(parser_data)
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank4]:     return _run_component(component, init.get(subcommand))
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank4]:     return component(**cfg)
[rank4]:            ^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 152, in setup
[rank4]:     fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank4]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank4]:     return to_run(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank4]:     return to_run(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/finetune/adapter.py", line 192, in main
[rank4]:     optimizer = instantiate_torch_optimizer(optimizer, model.parameters())
[rank4]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/users/industry/ai-hpc/apacsc41/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/utils.py", line 568, in instantiate_torch_optimizer
[rank4]:     optimizer["init_args"].update(kwargs)
[rank4]:     ~~~~~~~~~^^^^^^^^^^^^^
[rank4]: KeyError: 'init_args'
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[38643,1],7]
  Exit code:    1
--------------------------------------------------------------------------
[x1000c1s2b0n1:1326641] 15 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1000c1s2b0n1:1326641] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
