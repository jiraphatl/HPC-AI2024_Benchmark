{'access_token': None,
 'checkpoint_dir': PosixPath('/home/552/ps8597/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/552/ps8597/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x148a4d857d40>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/552/ps8597/scratch/workdir/llama_output/output_model/lora-bs128-mbs32'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO cudaDriverVersion 12040
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO Bootstrap : Using ib0:10.6.11.16<0>
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.6.11.16<0>
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO Using network IB
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO comm 0xd649780 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0xa032a8c57b9b2317 - Init START

gadi-gpu-v100-0080:2578533:2578533 [0] init.cc:871 NCCL WARN Duplicate GPU detected : rank 4 and rank 5 both on CUDA device 3d000
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1501 -> 5
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1746 -> 5
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1784 -> 5
NCCL version 2.20.5+cuda12.4
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO Using network IB
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO comm 0x10e243d0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3d000 commId 0xf0c3bfee0f45395 - Init START

gadi-gpu-v100-0080:2578533:2578533 [0] init.cc:871 NCCL WARN Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 3d000
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1501 -> 5
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1746 -> 5
gadi-gpu-v100-0080:2578533:2578533 [0] NCCL INFO init.cc:1784 -> 5
All GPUs are fully connected via NVLink.
Number of trainable parameters: 4,194,304
Number of non-trainable parameters: 6,738,415,616
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Using network IB
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO comm 0x12e54770 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x9131a14c8d308448 - Init START
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Setting affinity for GPU 0 to 01
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO NVLS multicast support is not available on dev 0
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO comm 0x12e54770 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Trees [0] 5/-1/-1->4->7 [1] 5/-1/-1->4->7
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO P2P Chunksize set to 131072
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Channel 00/0 : 4[0] -> 7[3] via P2P/CUMEM
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Channel 01/0 : 4[0] -> 7[3] via P2P/CUMEM
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Connected all rings
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO Connected all trees
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
gadi-gpu-v100-0080:2578533:2578624 [0] NCCL INFO comm 0x12e54770 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x9131a14c8d308448 - Init COMPLETE
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
Epoch 1 | iter 1 step 1 | loss train: 1.429, val: n/a | iter time: 84365.07 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.627, val: n/a | iter time: 82058.18 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.557, val: n/a | iter time: 81462.14 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 1.389, val: n/a | iter time: 82025.86 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 1.375, val: n/a | iter time: 84811.44 ms (step)
Training time: 430.23s
Memory used: 14.71 GB
Saving LoRA weights to '/home/552/ps8597/scratch/workdir/llama_output/output_model/lora-bs128-mbs32/final/lit_model.pth.lora'
gadi-gpu-v100-0080:2578533:2578630 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gadi-gpu-v100-0080:2578533:2581841 [0] NCCL INFO comm 0x12e54770 rank 4 nranks 8 cudaDev 0 busId 3d000 - Abort COMPLETE
