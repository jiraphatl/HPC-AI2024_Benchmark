{'access_token': None,
 'checkpoint_dir': PosixPath('/home/552/ps8597/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/552/ps8597/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1531a5f75af0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.1,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/552/ps8597/scratch/workdir/llama_output/output_model/lora-bs128-mbs32'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=10,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO Bootstrap : Using ib0:10.6.10.17<0>
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.20.5+cuda12.4
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.6.10.17<0>
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO Using network IB
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO comm 0xc4eb720 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0xa032a8c57b9b2317 - Init START

gadi-gpu-v100-0057:4151498:4151498 [0] init.cc:871 NCCL WARN Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 3d000
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1501 -> 5
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1746 -> 5
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1784 -> 5
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO Using network IB
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO comm 0xfcdcc90 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3d000 commId 0xd24cec3190ded5ef - Init START

gadi-gpu-v100-0057:4151498:4151498 [0] init.cc:871 NCCL WARN Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 3d000
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1501 -> 5
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1746 -> 5
gadi-gpu-v100-0057:4151498:4151498 [0] NCCL INFO init.cc:1784 -> 5
All GPUs are fully connected via NVLink.
Number of trainable parameters: 4,194,304
Number of non-trainable parameters: 6,738,415,616
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Using non-device net plugin version 0
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Using network IB
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO comm 0x11d2fc40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x9131a14c8d308448 - Init START
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Setting affinity for GPU 0 to 01
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO NVLS multicast support is not available on dev 0
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO comm 0x11d2fc40 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 00/02 :    0   3   6   5   4   7   2   1
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 01/02 :    0   3   6   5   4   7   2   1
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Trees [0] 1/-1/-1->0->3 [1] 1/-1/-1->0->3
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO P2P Chunksize set to 131072
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/CUMEM
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Connected all rings
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO Connected all trees
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
gadi-gpu-v100-0057:4151498:4151589 [0] NCCL INFO comm 0x11d2fc40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x9131a14c8d308448 - Init COMPLETE
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
Epoch 1 | iter 1 step 1 | loss train: 1.417, val: n/a | iter time: 84414.22 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.435, val: n/a | iter time: 82007.47 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.489, val: n/a | iter time: 81455.36 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 1.460, val: n/a | iter time: 82021.49 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 1.391, val: n/a | iter time: 84809.16 ms (step)
Training time: 430.21s
Memory used: 14.72 GB
Saving LoRA weights to '/home/552/ps8597/scratch/workdir/llama_output/output_model/lora-bs128-mbs32/final/lit_model.pth.lora'
{'checkpoint_dir': PosixPath('/home/552/ps8597/scratch/workdir/llama_output/output_model/lora-bs128-mbs32/final'),
 'precision': None,
 'pretrained_checkpoint_dir': None}
